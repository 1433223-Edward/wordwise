import os
import re

from collections import Counter
import streamlit as st
from PyPDF2 import PdfReader
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from streamlit import button

from chains import load_llm
from dotenv import load_dotenv
from sqlite import initialize_database
from sqlite import delete_newwords
from sqlite import exec_insert
from sqlite import exec_query
import nltk
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
import time
from streamlit_modal import Modal
import requests
from streamlit_autorefresh import st_autorefresh

load_dotenv(".env")
ollama_base_url = os.getenv("OLLAMA_BASE_URL")
llm_name = os.getenv("LLM")


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.complete_text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.complete_text += token

    def get_text(self):
        return self.complete_text


llm = load_llm(llm_name, config={"ollama_base_url": ollama_base_url})
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm=llm,memory=memory)
st.set_page_config(page_title="PDFè¯æ±‡å­¦ä¹ åŠ©æ‰‹", layout="wide")
initialize_database()
def ReadLinesAsList(file):
    with open(file,'r') as f:
        lines = f.readlines()
        vocab = {word.strip().split()[0].lower()
                 for word in lines if word.strip()}

        return vocab

# é¢„åŠ è½½è¯æ±‡è¡¨
@st.cache_data
def load_vocab_tables():
    """é¢„åŠ è½½CET6å’ŒCOCAè¯æ±‡è¡¨"""
    # CET6è¯è¡¨
    cet6_url = "https://raw.githubusercontent.com/mahavivo/english-wordlists/master/CET_4%2B6_edited.txt"
    coca_url = "https://raw.githubusercontent.com/mahavivo/english-wordlists/master/COCA_20000.txt"

    cet6_path = "/app/CET_4_6_edited.txt"
    coca_path = "/app/COCA_20000.txt"

    cet6_vocab = ReadLinesAsList(cet6_path)
    coca_vocab = ReadLinesAsList(coca_path)

    return cet6_vocab ,coca_vocab

# å…¨å±€å˜é‡å­˜å‚¨è¯æ±‡è¡¨
CET6_VOCAB, COCA_VOCAB = load_vocab_tables()
wnl = WordNetLemmatizer()

def get_word_category(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.VERB

def is_valid_word(word):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå•è¯"""
    word = word.lower()
    # è¿‡æ»¤ä¸“æœ‰åè¯å’ŒéCOCAè¯è¡¨å•è¯
    if not word in COCA_VOCAB or not word.isalpha():
        # å¿…é¡»å…¨æ˜¯å­—æ¯
        return False

    return True


def extract_words(text):
    """æå–å¹¶å¤„ç†æ–‡æœ¬ä¸­çš„å•è¯"""
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    valid_words = []
    for word in words:
        if len(word) > 1:  # è¿‡æ»¤å•å­—æ¯
            # è¯å½¢è¿˜åŸ
            if is_valid_word(word):
                valid_words.append(wnl.lemmatize(word, get_word_category(pos_tag([word])[0][1])))
    return valid_words


def display_word_card(word, explanation, freq, index, total):
    """æ˜¾ç¤ºå•è¯å¡ç‰‡"""
    st.markdown("""
    <style>
    .word-card {
        padding: 20px;
        border-radius: 10px;
        background-color: #f0f2f6;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)

    with st.container():
        st.markdown(f"<div class='word-card'>", unsafe_allow_html=True)
        st.markdown(f"### {word}")
        st.caption(f"åœ¨æ–‡æ¡£ä¸­å‡ºç° {freq} æ¬¡")
        st.markdown(explanation)
        st.caption(f"å¡ç‰‡ {index + 1}/{total}")
        st.markdown("</div>", unsafe_allow_html=True)


def get_word_explanations(words, llm):
    """ä½¿ç”¨LLMç”Ÿæˆå•è¯é‡Šä¹‰"""
    explanations = {}
    prompt_template = """è¯·ä¸ºä»¥ä¸‹è‹±è¯­å•è¯æä¾›ç®€æ˜çš„ä¸­æ–‡é‡Šä¹‰å’Œä¸€ä¸ªç®€çŸ­çš„ä¾‹å¥:
    å•è¯: {word}
    è¦æ±‚:
    1. ç»™å‡ºæœ€å¸¸ç”¨çš„1-2ä¸ªå«ä¹‰
    2. æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å¥
    3. è¾“å‡ºæ ¼å¼:
       é‡Šä¹‰: [ä¸­æ–‡é‡Šä¹‰]
       ä¾‹å¥: [è‹±æ–‡ä¾‹å¥]"""

    for word in words:
        container = st.empty()
        stream_handler = StreamHandler(container)
        prompt = prompt_template.format(word=word)
        llm.predict(prompt, callbacks=[stream_handler])
        explanations[word] = stream_handler.get_text()
        container.empty()  # æ¸…é™¤ä¸´æ—¶æ˜¾ç¤º
    return explanations


def main():


    if "explanations" not in st.session_state:
        st.session_state.explanations = {}
    if "word_freq" not in st.session_state:
        st.session_state.word_freq = None
    if "unknown_words_list" not in st.session_state:
        st.session_state.unknown_words_list = []


    st.title("ğŸ“š PDFè¯æ±‡å­¦ä¹ åŠ©æ‰‹")
    st.markdown("""
    è¿™æ˜¯ä¸€ä¸ªå¸®åŠ©ä½ æå‡è‹±è¯­è¯æ±‡é‡çš„å·¥å…·:
    1. ä¸Šä¼ è‹±è¯­PDFæ–‡æ¡£
    2. é€‰æ‹©ä½ çš„è¯æ±‡æ°´å¹³
    3. è·å–è¶…å‡ºä½ è¯æ±‡æ°´å¹³çš„å•è¯è§£é‡Šå’Œä¾‹å¥
    """)

    # ä¾§è¾¹æ é…ç½®

    # ä¸»ç•Œé¢
    pdf = st.file_uploader("ä¸Šä¼ è‹±è¯­PDFæ–‡æ¡£", type="pdf")

    if pdf is not None:
        with st.spinner('æ­£åœ¨åˆ†ææ–‡æ¡£...'):
            # è¯»å–PDF
            pdf_reader = PdfReader(pdf)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()

            # åˆ†æè¯æ±‡
            words = extract_words(text)
            st.session_state.word_freq = Counter(words)
            unknown_words = {word for word in st.session_state.word_freq.keys()
                             if word not in CET6_VOCAB}

            if unknown_words:
                # ä»…åœ¨é¦–æ¬¡ä¸Šä¼ PDFæ—¶ç”Ÿæˆé‡Šä¹‰
                if not st.session_state.explanations:
                    with st.spinner('æ­£åœ¨ç”Ÿæˆå•è¯é‡Šä¹‰...'):
                        st.session_state.explanations = get_word_explanations(unknown_words, llm)
                    st.session_state.unknown_words_list = sorted(unknown_words)


                for word in unknown_words:

                    insert_sql = "INSERT INTO new_words (word , explanations,insert_time) VALUES (?,?,?)"
                    cur_time = int(time.time()*1000)
                    res=exec_insert(insert_sql,(word,st.session_state.explanations[word] ,cur_time))
                    if res:
                        print("INSERT ERROR:{}".format(res))


                # å¯¼èˆªæŒ‰é’®
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª"):
                        st.session_state.current_word_index = (st.session_state.current_word_index - 1) % len(
                            st.session_state.unknown_words_list)
                with col2:
                    if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª"):
                        st.session_state.current_word_index = (st.session_state.current_word_index + 1) % len(
                            st.session_state.unknown_words_list)
                with col3:
                    if st.button("ğŸ”„ é‡ç½®"):
                        st.session_state.current_word_index = 0

                # ä»ç¼“å­˜ä¸­è·å–å¹¶æ˜¾ç¤ºå½“å‰å•è¯å¡ç‰‡
                current_word = st.session_state.unknown_words_list[st.session_state.current_word_index]
                display_word_card(
                    current_word,
                    st.session_state.explanations[current_word],
                    st.session_state.word_freq[current_word],
                    st.session_state.current_word_index,
                    len(st.session_state.unknown_words_list)
                )

def recall_new_words():
    num_words = st.session_state.num_new_words
    st.header("éœ€è¦å¤ä¹ çš„ç”Ÿè¯:{}".format(num_words))

    query_sql = "SELECT word , explanations FROM new_words ORDER BY insert_time LIMIT ?"

    res = exec_query(query_sql,(num_words,))
    new_words=[]
    explanations={}

    for row in res:
        new_words.append(row[0])
        explanations[row[0]]=row[1]

    print("éœ€è¦å¤ä¹ çš„å•è¯ï¼š")
    print(new_words)
    print(explanations)
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª"):
            st.session_state.current_word_index = (st.session_state.current_word_index - 1) % len(
                new_words)
    with col2:
        if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª"):
            print("ä¸‹ä¸€ä¸ª:{}".format(st.session_state.current_word_index))
            st.session_state.current_word_index = (st.session_state.current_word_index + 1) % len(
               new_words)
    with col3:
        if st.button("ğŸ”„ é‡ç½®"):
            st.session_state.current_word_index = 0

    # ä»ç¼“å­˜ä¸­è·å–å¹¶æ˜¾ç¤ºå½“å‰å•è¯å¡ç‰‡
    current_word = new_words[st.session_state.current_word_index]
    display_word_card(
        current_word,
        explanations[current_word],
        1,
        st.session_state.current_word_index,
        len(new_words)
    )




if __name__ == "__main__":

   if "page" not in st.session_state:
       st.session_state.page = "PDF"
   if "num_new_words" not in st.session_state:
        st.session_state.num_new_words=0
   if "chat_history" not in st.session_state:
       st.session_state["chat_history"] = []
   if "current_word_index" not in st.session_state:
       st.session_state.current_word_index = 0

   interval=1
   main_container = st.empty()
   modal = Modal(key="Demo Key",title="è­¦å‘Š")
   page = st.selectbox("é€‰æ‹©åŠŸèƒ½",["PDF" ,"å¤ä¹ å•è¯"])

   if page== "PDF":
       st.session_state.page="PDF"
   elif page=="å¤ä¹ å•è¯":
       if st.session_state.num_new_words > 0:
          
            st.session_state.page="recall_words"
   with st.sidebar:
       st.header("è®¾ç½®")
       level = st.selectbox(
           "é€‰æ‹©ä½ çš„è¯æ±‡æ°´å¹³",
           ["CET6"],
           help="ç›®å‰ä»…æ”¯æŒCET6æ°´å¹³æ£€æµ‹"
       )


       num_words = int(st.number_input("è¾“å…¥è¦å¤ä¹ çš„ç”Ÿè¯æ•°é‡"))
       st.session_state.num_new_words = num_words

       user_input = st.text_input("å•è¯å°åŠ©æ‰‹")

       if user_input:
           st.session_state["chat_history"].append(f"ç”¨æˆ·è¾“å…¥:{user_input}")

           response = conversation.predict(input=user_input)

           st.session_state["chat_history"].append(f"AIåŠ©æ‰‹:{response}")

       for message in st.session_state["chat_history"]:
            st.write(message)

   if st.session_state.page== "PDF":
       main()
   elif st.session_state.page== "recall_words":
       recall_new_words()
